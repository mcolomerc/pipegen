# {{.ProjectNameTitle}}

A streaming data pipeline built with Apache Kafka, FlinkSQL, and AVRO schemas, generated by PipeGen.

## Project Overview

This project implements a real-time data processing pipeline that:
- Consumes events from Kafka topics
- Processes data using FlinkSQL transformations
- Produces results to output topics
- Uses AVRO schemas for structured data serialization

## Project Structure

```
{{.ProjectName}}/
├── README.md                   # This documentation
├── .pipegen.yaml              # PipeGen configuration file
├── docker-compose.yml         # Local development stack
├── flink-conf.yaml            # Flink cluster configuration
├── sql/                       # FlinkSQL processing statements
│   ├── 01_create_source_table.sql     # Input data source table
│   ├── 02_create_processing.sql       # Data transformation logic
│   ├── 03_create_output_table.sql     # Output data table definition
│   └── 04_insert_results.sql          # Insert processed results
└── schemas/                   # AVRO schema definitions
    ├── input_event.avsc       # Input event schema
    └── output_result.avsc     # Output result schema
```

## Generated Resources

### AVRO Schemas
- **Input Schema**: `{{.SanitizedName}}.events.InputEvent` - Defines the structure of incoming events
- **Output Schema**: `{{.SanitizedName}}.results.OutputResult` - Defines the structure of processed results

### Kafka Topics
- **Input Topic**: `input-events` - Receives raw event data
- **Output Topic**: `output-results` - Contains processed results

### FlinkSQL Jobs
1. **Source Table**: Creates Kafka source table for input events
2. **Processing Logic**: Implements data transformations and aggregations
3. **Output Table**: Creates Kafka sink table for results
4. **Results Insert**: Inserts processed data into output topic

## Quick Start

### 1. Deploy Local Development Stack

Start Kafka, Flink, and Schema Registry services:

```bash
pipegen deploy
```

This will start:
- **Kafka Broker** (localhost:9092) - Message streaming platform
- **Flink JobManager** (http://localhost:8081) - Stream processing cluster
- **Flink TaskManager** - Stream processing workers
- **Schema Registry** (http://localhost:8082) - AVRO schema management

### 2. Run the Pipeline

Execute the complete streaming pipeline:

```bash
# Basic execution (5 minutes, 100 msg/sec)
pipegen run

# Custom execution with parameters
pipegen run --message-rate 500 --duration 2m

# Run with live dashboard monitoring
pipegen run --dashboard --message-rate 200 --duration 10m

# Show execution plan without running
pipegen run --dry-run
```

### 3. Monitor with Dashboard

Start the live monitoring dashboard:

```bash
# Dashboard with pipeline execution
pipegen dashboard

# Standalone dashboard (monitor existing services)
pipegen dashboard --standalone

# Custom port
pipegen dashboard --port 8080
```

The dashboard provides:
- Real-time pipeline metrics and status
- FlinkSQL job execution monitoring
- Kafka topic and message throughput
- Producer and consumer performance
- Interactive pipeline flow visualization
- Error tracking and resolution suggestions

## Configuration

### Environment Variables

Set these environment variables to override defaults:

```bash
# Kafka Configuration
export PIPEGEN_BOOTSTRAP_SERVERS="localhost:9092"
export PIPEGEN_SCHEMA_REGISTRY_URL="http://localhost:8082"
export PIPEGEN_FLINK_URL="http://localhost:8081"

# Pipeline Settings
export PIPEGEN_DEFAULT_MESSAGE_RATE=100
export PIPEGEN_DEFAULT_DURATION="5m"
export PIPEGEN_TOPIC_PREFIX="{{.ProjectName}}"
export PIPEGEN_CLEANUP_ON_EXIT=true

# Dashboard Configuration
export PIPEGEN_DASHBOARD_PORT=3000
export PIPEGEN_DASHBOARD_AUTO_OPEN=true
```

## Service URLs

When the local stack is running:

- **Flink Web UI**: http://localhost:8081 - Monitor jobs, checkpoints, and metrics
- **Schema Registry**: http://localhost:8082 - Manage AVRO schemas
- **PipeGen Dashboard**: http://localhost:3000 - Real-time pipeline monitoring

## Next Steps

- **Production Deployment**: Update connection strings for cloud Kafka/Flink clusters
- **Schema Evolution**: Use Schema Registry for schema versioning and compatibility
- **Monitoring**: Integrate with Prometheus/Grafana for production monitoring
- **Scaling**: Configure auto-scaling for Flink TaskManagers based on load
- **Security**: Enable SASL authentication and SSL encryption for production

Generated by PipeGen v1.0 - https://github.com/mcolomerc/pipegen
